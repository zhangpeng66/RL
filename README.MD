#  <center>RL</center>

## 安装与运行

离线运行
lerobot-dataset-viz  --repo-id lerobot/pusht   --root /home/ahpc/.cache/huggingface/lerobot/lerobot/pusht   --mode local  --episode-index 0

问题：运行命令python -m lerobot.rl.gym_manipulator --config_path path/to/gym_hil_env.json报错 ImportError: numpy.core.multiarray failed to import
解决方法：pip install opencv-python==4.12.0.88

打开仿真环境，在配置文件设置如下：
1、mode = null
2、PandaPickCubeBase-v0: Basic environment
   PandaPickCubeGamepad-v0: With gamepad control   蓝牙手柄
   PandaPickCubeKeyboard-v0: With keyboard control 键盘
python -m lerobot.rl.gym_manipulator --config_path /home/ahpc/RL/sim_test/configs/gym_hil_env.json
出现查找不到设备的时候崩溃现象，修改如下：
```C++
   # Get action features from teleop device or environment action space
   if teleop_device is not None:
      action_features = teleop_device.action_features
   else:
      # For gym_hil environments without teleop device, derive from action space
      action_space = env.action_space
      print("Action space:", action_space)
      action_features = {
            "dtype": "float32",
            "shape": action_space.shape,
            "names": None,
      }
```
需要键盘控制的时候需要先按下**空格健**使能控制
Keyboard controls:
  Arrow keys: Move in X-Y plane
  Shift and Shift_R: Move in Z axis
  Right Ctrl and Left Ctrl: Open and close gripper
  Enter: End episode with SUCCESS
  Backspace: End episode with FAILURE
  Space: Start/Stop Intervention
  ESC: Exit


## 传统机器人技术
参考知乎[机器学习](https://zhuanlan.zhihu.com/p/1961959949446936245)
### 显式模型与隐式模型
机器人运动生成方法的范围从传统显式模型（**即基于动力学的方法**）到隐式模型（**即基于学习的方法**）：前者利用机器人刚体力学及其与环境中潜在障碍物交互的精确描述；后者则将人工运动视为在给定多个感觉运动读数时需要学习的统计模式。
要理解为何基于学习的方法在机器人社区中日益流行，需要简要分析操作任务的传统方法——这些方法利用正运动学（FK）、逆运动学（IK）和控制理论等工具。

![alt text](image.png)

### 基于动力学的机器人技术的局限性
但自主机器人在物理世界中仍远不能以人类水平的性能完成任务，尤其在（1）跨机器人载体（不同操作器、不同运动平台等）和（2）跨任务（系鞋带、操作多种物体等）的泛化能力方面存在显著不足。上述基于动力学的方法虽在机器人技术早期发展中至关重要，但在实际应用中需要大量人类专业知识，且通常仅针对特定应用问题设计。

基于动力学的机器人技术流水线历来采用“顺序开发”模式：如今大多数架构中的不同模块（**传感、状态估计、映射、规划、（微分）逆运动学、底层控制**），最初都是为特定用途独立开发的，且接口固定。将这些专用模块组装成流水线的过程容易出错：当发生变化时（例如，传感环节的光照变化、传感器遮挡/故障、控制失效），系统会表现出脆性，且误差会不断累积。要将这样的技术栈适配到新任务或新机器人平台，往往需要在多个阶段重新定义目标、约束和启发式规则，导致巨大的工程开销

综合来看，这些局限性推动了对基于学习方法的探索——这类方法能够（1）更紧密地集成感知与控制；（2）减少专家建模干预，实现跨任务和跨载体的自适应；（3）随着更多机器人数据的可得，性能能够平稳扩展

## Robot(Reinforcement)Learning
基于学习的方法可完全绕过显式建模，仅依赖交互数据即**依赖一体化的“预测-动作”流水线（视觉运动策略）**——————这一优势在动力学难以建模或完全未知的场景中具有变革性。最后，机器人学习（Robot Learning）天生适合利用日益增多的开放可用机器人数据，正如计算机视觉和自然语言处理领域历史上从大规模语料库中获益那样，而这在很大程度上被基于动力学的方法所忽视。
作为一个相对新兴的领域，机器人学习尚未出现某种技术明显优于其他技术的局面。尽管如此，有两类方法已获得广泛关注：RL和行为克隆（Behavior Clone ,BC）
![alt text](image-1.png)   

通用机器人模型与单任务行为克隆方法并列呈现。尽管从本质上看，二者存在显著差异——通用模型受语言条件约束，可通过指令生成适用于多种任务的运动；而单任务模型通常不受语言条件约束，仅用于执行单一任务——但基础模型的训练在很大程度上仍依赖于“从（大型）输入演示训练集中复现轨迹”。因此，我们认为通用策略确实可与其他单任务行为克隆方法归为一类，因为它们均采用相似的训练数据和模式。

列出了当前lerobot中可用的所有机器人学习策略：基于Transformer的动作分块（ACT）、扩散策略（Diffusion Policy）、向量量化行为Transformer（VQ-BeT）、SmolVLA、人在环样本高效强化学习（HIL-SERL）以及TD-MPC。

### 强化学习（简明）介绍
强化学习是机器学习的一个子领域，核心目标是agent（智能体）在进行某个任务action时（决策），首先与environment进行交互，产生新的状态state，同时环境给出奖励reward，如此循环下去，agent和environment不断交互产生更多新的数据。强化学习算法就是通过一系列动作策略与环境交互，产生新的数据，再利用新的数据去修改自身的动作策略，经过数次迭代后，agent就会学习到完成任务所需要的动作策略。
![alt text](image-7.png)

形式上，智能体与环境之间的交互通常通过**马尔可夫决策过程（MDP） 建模**。将机器人问题通过马尔可夫决策过程表示具有多项优势，包括（1）通过其固有的随机形式纳入不确定性；（2）为“无需环境动力学显式模型的学习”提供理论严谨的框架。尽管马尔可夫决策过程也可支持连续时间形式，但在强化学习中通常考虑离散时间场景，假设交互在离散时间步t = 0,1,2,3,4.....T发生。允许无限次交互（$T\rightarrow\infty$）的马尔可夫决策过程被称为“无限时域”（infinite-horizon）马尔可夫决策过程，与之相对的是“有限时域”（finite-horizon）马尔可夫决策过程（T为有限值）。除非特别说明，本文仅讨论离散时间有限时域（ episodic，片段式）马尔可夫决策过程。
长度为T的马尔可夫决策过程（MDP）是一个元组，有如下定义：
S为状态空间；$s_t\in S$表示时间t时环境的状态。在机器人技术中，状态通常包括机器人配置和速度，还可包含传感器读数（如摄像头或音频流）。
A为动作空间；$a_t \in A$可表示t时间步时的关节力矩、关节速度，甚至末端执行器指令。通常，动作对应于“干预机器人配置的指令”。
D表示（可能非确定性的）环境动力学，有$D(s_t,a_t,s_{t+1}) = P(s_{t+1}| s_t,a_t)$,表示从一个状态转移到另一个状态的概率分布
r为奖励函数，描述了在状态$s_t$下执行$a_t$的期望即时奖励$\mathcal{R}(s,a)=\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]$
$\gamma$为折扣因子，用于调节对**即时奖励与长期奖励的偏好**

### 机器人的真实世界强化学习
端到端的简化控制流水线、数据驱动的特征提取、“无需显式建模而依赖交互数据”——这些都是机器人强化学习的核心特征。然而，强化学习在安全性和学习效率方面仍存在局限性，这对真实世界机器人应用而言尤为关键。    

（1）首先，尤其在训练初期，智能体的动作通常具有探索性，可能表现出随机性。在物理系统上，未训练完成的策略可能会发出“高速度、自碰撞配置或超过关节极限的力矩”等指令，导致设备磨损甚至硬件损坏。要降低这些风险，需引入外部保护机制（如监控器、安全监测器、紧急停止装置），而这往往需要大量人工监督。
（2）此外，在大多数机器人问题采用的“片段式训练”场景中，训练过程中需手动重置环境，这一过程耗时且易出错，大幅降低了实验效率。其次，强化学习的“样本效率低”问题仍未解决——由于训练时间过长，限制了其在真实世界机器人中的适用性。即使是SAC等性能优良的算法，通常也需要大量的状态转移样本；而在真实硬件上生成这些数据非常耗时。     

在模拟器中训练强化学习策略可同时解决上述两个问题：既消除了物理风险，又大幅提高了数据生成效率。
（1）然而，构建和维护“真实世界问题的高保真模拟器”难度极大——尤其对于“接触密集型操作”和“涉及可变形或软材料的任务”。
（2）奖励设计是真实世界强化学习流水线中“脆弱性”的另一个重要来源。尽管为“长时域任务中的探索引导”设计密集奖励通常是必要的，但这一过程易出错且严重依赖人类专业知识和直觉。设计不当的奖励项可能导致“策略投机”（specification gaming）或收敛到局部最优；
通过“从大量人类演示中学习”的行为克隆（BC）技术的进步，可同时解决上述两个问题。尽管行为克隆存在“固有次优性”（模仿学习的性能最多只能与演示者持平），但“通过行为克隆复现专家演示”的方法已被证明具有越来越强的竞争力和实用性——无需依赖仿真环境，也无需设计难以定义的奖励函数。
![alt text](image-5.png)

### 代码示例：真实世界强化学习
从高层架构看，HIL-SERL（图17）依赖两个核心组件：
执行器（Actor）：运行“冻结的策略网络”以与环境交互并获取观测。观测既用于“约束冻结执行器选择待执行动作”，也用于构建状态转移样本$(s_t,a_t,r_t,s_{t+1})$，并与学习器共享。奖励通过“基于离线演示数据集训练的自定义奖励分类器”推断得到。
学习器（Learner）：用于优化策略参数$\theta$，以最大化期望回报。学习器从在线和离线缓冲区中按等比例采样批量数据，并将更新后的参数共享给执行器。
本示例中的HIL-SERL架构可在本地运行，而lerobot的实现还支持“执行器和学习器在两台通过网络连接的机器上分别运行”。
![alt text](image-6.png)